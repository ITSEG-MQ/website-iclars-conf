---
layout: default
title: Talk I
nav: archive
---

<h2 style="line-height:1.3;">How Safe Will I Be Given What I See? Visual Prediction of Calibrated Safety Chances with (Foundation) World Models</h2>

<p><b>Speaker:</b> Prof. Ivan Ruchkin</p>

<h3>Abstract</h3>
<p>
  In safety-critical autonomous systems, safety prediction traditionally relies on low-dimensional data with specific physical meanings, such as poses and velocities. However, such data is not always available, which leaves only high-dimensional sensor observations, such as images from cameras or LiDAR scans, and makes safety prediction increasingly challenging. This talk reports on the recent techniques for using high-dimensional observation data for safety prediction; at the heart of these techniques is the notion of a world model, which can predict future observations without meaningful low-dimensional data. We present several world models implemented with neural representation learning as well as foundation models for image segmentation and natural language prediction. Additionally, we propose a novel uncertainty quantification technique that combines confidence calibration with conformal prediction.  
</p>

<h3>Brief Bio</h3>
<p>
  At the University of Florida, Ivan Ruchkin lead the Trustworthy Engineered Autonomy (TEA) Lab. His research aims to make autonomous systems safer and more trustworthy. To this end, He develop techniques, tools, and methodologies for modeling, analyzing, verifying, controlling, and monitoring autonomous robotic and cyber-physical systems.  
</p>
